{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "9178afb9b3daeee402732af4e4aef10a7593df5184141cef26a75ebe4e69e442"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando bibliotecas auxiliares\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "from random import seed\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "initial_weights:\n ['0.42000, 0.15000, 0.40000; 0.72000, 0.10000, 0.54000; 0.01000, 0.19000, 0.42000; 0.30000, 0.35000, 0.68000\\n', '0.21000, 0.67000, 0.14000, 0.96000, 0.87000; 0.87000, 0.42000, 0.20000, 0.32000, 0.89000; 0.03000, 0.56000, 0.80000, 0.69000, 0.09000\\n', '0.04000, 0.87000, 0.42000, 0.53000; 0.17000, 0.10000, 0.95000, 0.69000  ']\ndataset:\n [['0.32000, 0.68000; 0.75000, 0.98000'], ['0.83000, 0.02000; 0.75000, 0.28000']]\ndef_network:\n ['0.250', '2', '4', '3', '2']\n"
     ]
    }
   ],
   "source": [
    "# lendo os arquivos de entrada\n",
    "def read_initial_weights(path_file):\n",
    "    '''função responsavel em ler o arquivo de pesos inicias'''\n",
    "    return open(path_file,'r+').readlines()\n",
    "\n",
    "def read_dataset(path_file):\n",
    "    '''função responsável em ler o arquivo de dataset'''\n",
    "    dataset = list()\n",
    "    file = open(path_file, 'r+')\n",
    "    for lines in file:\n",
    "        values_line = list()\n",
    "        dataset.append([lines.rstrip()])\n",
    "    return dataset\n",
    "    \n",
    "def read_def_network(path_file):\n",
    "    '''função responsável em ler o arquivo de definição da rede neural'''\n",
    "    file = open(path_file, \"r+\")\n",
    "    \n",
    "    network = list()\n",
    "    for line in file:\n",
    "        network.append(line.rstrip())\n",
    "    return network\n",
    "\n",
    "initial_weights = read_initial_weights('initial_weights.txt')\n",
    "print('initial_weights:\\n', initial_weights)\n",
    "\n",
    "dataset = read_dataset(\"data_teste.txt\")\n",
    "print('dataset:\\n', dataset)\n",
    "\n",
    "def_network = read_def_network('network.txt')\n",
    "print('def_network:\\n', def_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a arquitetura da rede\n",
    "\n",
    "def create_network(def_network, initial_weights):\n",
    "    '''função responsável em criar a rede neural a partir dos arquivos de definição'''\n",
    "    '''def_network definição da estrutura da rede'''\n",
    "    '''initial_weights pesos inicias da rede'''\n",
    "    network = list()\n",
    "    \n",
    "    # trabalhando sobre o arquivo que define a rede\n",
    "    regulation = def_network[0]\n",
    "    def_network.pop(0) # remove a regulação já armazenada\n",
    "    \n",
    "    input_length = def_network[0]\n",
    "    def_network.pop(0)\n",
    "    \n",
    "    output_length = def_network[len(def_network)-1]\n",
    "    def_network.pop(len(def_network)-1)\n",
    "    \n",
    "    # imprimindo informações capturadas da definição da rede\n",
    "    print('[INFO] Regulação: ', regulation)\n",
    "    print('[INFO] Quantidade de entradas: ', input_length)\n",
    "    print('[INFO] Quantidade de saídas: ', output_length)\n",
    "    # imprimindo informações das camadas ocultas\n",
    "    print('[INFO] Número de neurônios em cada camada oculta: ', def_network, '\\n')\n",
    "    \n",
    "    # trabalhando sobre o arquivo de quefine os pesos\n",
    "    for weights in initial_weights:\n",
    "        layers = weights.rstrip().split('; ')\n",
    "        network_layer = list()\n",
    "        for neuron in layers:\n",
    "            if ', ' in neuron:\n",
    "                network_layer.append({'weights': neuron.rstrip().split(', ')})\n",
    "            else:\n",
    "                network_layer.append(neuron)\n",
    "        network.append(network_layer)\n",
    "    print('[INFO] Rede neural criada: ', network, '\\n')\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] Regulação:  0.250\n[INFO] Quantidade de entradas:  2\n[INFO] Quantidade de saídas:  2\n[INFO] Número de neurônios em cada camada oculta:  ['4', '3'] \n\n[INFO] Rede neural criada:  [[{'weights': ['0.42000', '0.15000', '0.40000']}, {'weights': ['0.72000', '0.10000', '0.54000']}, {'weights': ['0.01000', '0.19000', '0.42000']}, {'weights': ['0.30000', '0.35000', '0.68000']}], [{'weights': ['0.21000', '0.67000', '0.14000', '0.96000', '0.87000']}, {'weights': ['0.87000', '0.42000', '0.20000', '0.32000', '0.89000']}, {'weights': ['0.03000', '0.56000', '0.80000', '0.69000', '0.09000']}], [{'weights': ['0.04000', '0.87000', '0.42000', '0.53000']}, {'weights': ['0.17000', '0.10000', '0.95000', '0.69000']}]] \n\n[INFO] valor de saída (fx):  [0.8295270280667102, 0.8383188927203041] \n\n"
     ]
    }
   ],
   "source": [
    "def dot(v, w):\n",
    "    return sum(float(v_i) * float(w_i) for v_i, w_i in zip(v, w))\n",
    "\n",
    "def sigmoid(activation):\n",
    "    '''função responsável por calcular a aproximação da função'''\n",
    "    return 1 / (1 + math.exp(-activation))\n",
    "\n",
    "def forward_propagate(network, row):\n",
    "    ''''''\n",
    "    inputs = row\n",
    "    \n",
    "    index = 0\n",
    "    for layer in network:\n",
    "        new_inputs = list()\n",
    "        if index != len(network)-1:\n",
    "            new_inputs.append(1.00000) # adicionando a entrada do vies\n",
    "        for neuron in layer:\n",
    "            activation = dot(neuron['weights'], inputs)\n",
    "            neuron['output'] = sigmoid(activation)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "        index = index + 1\n",
    "    return inputs\n",
    "\n",
    "network = create_network(def_network, initial_weights)\n",
    "row = [1.00000, 0.83000, 0.02000]\n",
    "output = forward_propagate(network, row)\n",
    "print('[INFO] valor de saída (fx): ', output, '\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] J do exemplo 2: 1.9437823352945296\n"
     ]
    }
   ],
   "source": [
    " # Função de custo\n",
    " \n",
    " def custo(fx, y):\n",
    "        \"\"\"\n",
    "        fx: Valores preditos\n",
    "        y: Saída original\n",
    "\n",
    "        \"\"\"\n",
    "        j = -((y * np.log(fx) + (1 - y) * np.log(1 - fx)))\n",
    "        return j.sum()\n",
    "\n",
    "# Saídas são listas\n",
    "fx = output\n",
    "y = [0.75000, 0.28000]\n",
    "\n",
    "# Transforma para arrays\n",
    "fx_array = np.array(fx)\n",
    "y_array = np.array(y)\n",
    "\n",
    "funcao_custo = custo(fx_array, y_array)\n",
    "\n",
    "print('[INFO] J do exemplo 2:', funcao_custo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'regulation' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-153-4b8945f3eee8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mregularization_factor\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnum_examples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mteste\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcusto_total\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregulation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'regulation' is not defined"
     ]
    }
   ],
   "source": [
    "# Custo total do dataset\n",
    "\n",
    "def custo_total(regulation, num_exemplos):\n",
    "        return (regularization_factor/(2*num_examples)) * np.sum(np.square(w[:, 1:]).sum() for w in weights)\n",
    "\n",
    "teste = custo_total(regulation, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_derivative(output):\n",
    "\treturn output * (1.0 - output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "errors [-0.07952702806671019, -0.5583188927203041]\nNone\n"
     ]
    }
   ],
   "source": [
    "# Calcular os Deltas\n",
    "\n",
    "\n",
    "def deltas(fx, y):\n",
    "        # Set output layer separately\n",
    "        np.subtract(fx, y, out=self.deltas[-1])\n",
    "        for layer in range(self.num_layers-2, 0, -1):\n",
    "            weights = self.weights[layer].transpose()[1:]\n",
    "            uncertainty = np.multiply(self.activations[layer][1:], (1 - self.activations[layer][1:]))\n",
    "            np.multiply(np.dot(weights, self.deltas[layer]), uncertainty, out=self.deltas[layer-1])\n",
    "\n",
    "\n",
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network)-1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.250           # Coloquei o fator de regularização aqui, não sei ainda se deve deixar\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (float(neuron['weights'][j]) * neuron['delta'])\n",
    "                    errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(expected[j] - neuron['output'])\n",
    "            print('errors', errors)\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    "            #print('delta', neuron)\n",
    "\n",
    "\n",
    "saida1 = [0.75000, 0.28000]\n",
    "\n",
    "teste2 = backward_propagate_error(network, saida1)\n",
    "print(teste2)\n",
    "\n",
    "# pq tá negativo, o esse errors/delta tá cravando os resultados mas tá negativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-6c4f0c201b8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m# construindo a rede neural\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mnetwork\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdef_network\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_weights\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# TODO: mudar para o arquivo def a rede\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1.00000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.13000\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# adicionando a entrada do vies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_propagate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-50-73a8d2977e58>\u001b[0m in \u001b[0;36mcreate_network\u001b[1;34m(def_network, initial_weights)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mdef_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0moutput_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdef_network\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdef_network\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mdef_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdef_network\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "    return output * (1.0 - output)\n",
    "\n",
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network)-1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (float(neuron['weights'][j]) * neuron['delta'])\n",
    "                    errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(expected[j] - neuron['output'])\n",
    "            print('errors', errors)\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    "\n",
    "def update_weights(network, row, l_rate):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] += l_rate * neuron['delta']\n",
    "            \n",
    "# construindo a rede neural\n",
    "network = create_network(def_network, initial_weights)  # TODO: mudar para o arquivo def a rede\n",
    "row = [1.00000, 0.13000] # adicionando a entrada do vies\n",
    "output = forward_propagate(network, row)\n",
    "print('[INFO] valor de saída (fx): ', output, '\\n')\n",
    "\n",
    "backward_propagate_error(network, [0.90000])\n",
    "\n",
    "# visualizando a rede com os pesos calculados\n",
    "for layer in network:\n",
    "    for neuron in layer:\n",
    "        print(neuron)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network.  For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly, using a Gaussian\n",
    "        distribution with mean 0, and variance 1.  Note that the first\n",
    "        layer is assumed to be an input layer, and by convention we\n",
    "        won't set any biases for those neurons, since biases are only\n",
    "        ever used in computing the outputs from later layers.\"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The ``training_data`` is a list of tuples\n",
    "        ``(x, y)`` representing the training inputs and the desired\n",
    "        outputs.  The other non-optional parameters are\n",
    "        self-explanatory.  If ``test_data`` is provided then the\n",
    "        network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for\n",
    "        tracking progress, but slows things down substantially.\"\"\"\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in xrange(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in xrange(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print \"Epoch {0}: {1} / {2}\".format(\n",
    "                    j, self.evaluate(test_data), n_test)\n",
    "            else:\n",
    "                print \"Epoch {0} complete\".format(j)\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "        is the learning rate.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  }
 ]
}