{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "9178afb9b3daeee402732af4e4aef10a7593df5184141cef26a75ebe4e69e442"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Benchmark"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from random import seed\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_teste = open(\"data_teste.txt\",\"r\")\n",
    "\n",
    "#print(data_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar(df, class_column):\n",
    "    x = df.drop(class_column, axis=1) # remove classe\n",
    "    #if normalize == 'minmax':\n",
    "    x = (x-x.min())/(x.max()-x.min()) # normalize\n",
    "    #elif normalize == 'mean':\n",
    "    #    x = (x-x.mean())/x.std()\n",
    "    normalized_attr = x.fillna(0) # replace NaN's with 0's\n",
    "    return pd.concat([normalized_attr, df[class_column]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_dataset(dataset):\n",
    "    d = []\n",
    "    num_features = 0\n",
    "    num_outputs = 0\n",
    "    with open(dataset, 'r') as f:\n",
    "        for line in f.read().splitlines():\n",
    "            x, y = line.split(';')\n",
    "            x = [float(i) for i in x.replace(',',' ').split()]\n",
    "            y = [float(i) for i in y.replace(',',' ').split()]\n",
    "            num_features = len(x)\n",
    "            num_outputs = len(y)\n",
    "            d.append(x + y)\n",
    "    return pd.DataFrame(d, columns=['x'+str(i) for i in range(num_features)]+['y'+str(i) for i in range(num_outputs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "     x0    x1    y0    y1\n",
      "0  0.32  0.68  0.75  0.98\n",
      "1  0.83  0.02  0.75  0.28\n",
      "[[0.32 0.68]\n",
      " [0.83 0.02]]\n"
     ]
    }
   ],
   "source": [
    "df = txt_dataset(\"data_teste.txt\")\n",
    "print(df)\n",
    "\n",
    "x = df.drop([c for c in df.columns if c.startswith('y')], axis=1).values\n",
    "y = df.drop([c for c in df.columns if c.startswith('x')], axis=1).values\n",
    "class_values = None\n",
    "\n",
    "print(x)\n",
    "\n",
    "#normalized_df = normalizar(data1, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'[(0.75, 0.98) (0.75, 0.28)] not found in axis'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-81a3a85b4a73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnormalized_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalizar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-9522f7194ddc>\u001b[0m in \u001b[0;36mnormalizar\u001b[1;34m(df, class_column)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mnormalizar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_column\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_column\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# remove classe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;31m#if normalize == 'minmax':\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# normalize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#elif normalize == 'mean':\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4160\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[1;36m1.0\u001b[0m     \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4161\u001b[0m         \"\"\"\n\u001b[1;32m-> 4162\u001b[1;33m         return super().drop(\n\u001b[0m\u001b[0;32m   4163\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4164\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3882\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3883\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3884\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3885\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3886\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3916\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3917\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3918\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3919\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3920\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   5276\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5277\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5278\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5279\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5280\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '[(0.75, 0.98) (0.75, 0.28)] not found in axis'"
     ]
    }
   ],
   "source": [
    "normalized_df = normalizar(df, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler arquitetura da rede a partir do arquivo txt\n",
    "\n",
    "def build_architecture(architecture):\n",
    "        with open(architecture, 'r') as f:\n",
    "            fileread = f.read().splitlines()\n",
    "        regularization_factor = float(fileread[0])\n",
    "        architecture = [int (x) for x in fileread[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura dos pesos iniciais\n",
    "\n",
    "def read_weights(file):\n",
    "        w = []\n",
    "        with open(file, 'r') as f:\n",
    "            for line in f:\n",
    "                w.append([float(x) for x in line.replace(';',',').split(',')])\n",
    "        weights = np.array([np.asmatrix(w[layer]).reshape(architecture[layer+1], architecture[layer]+1) for layer in range(num_layers-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Função de custo\n",
    " \n",
    " def custo(fx, y):\n",
    "        \"\"\"\n",
    "        fx (np.array): Valores preditos\n",
    "        y (np.array): Saída original\n",
    "\n",
    "        \"\"\"\n",
    "        j = -((y * np.log(fx) + (1 - y) * np.log(1 - fx)))\n",
    "        return j.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-e2362e903b22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtxt2dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mclass_column\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "    if args.data.endswith('.txt'):\n",
    "        df = txt2dataframe(args.data)\n",
    "    else:\n",
    "        df = pd.read_csv(args.data, sep=args.sep)\n",
    "    class_column = args.class_column\n",
    "\n",
    "    for column in args.drop:\n",
    "        df.drop(column, inplace=True, axis=1)\n",
    "\n",
    "    if args.data.endswith('.txt'):  # naive dataset txt\n",
    "        x = df.drop([c for c in df.columns if c.startswith('y')], axis=1).values\n",
    "        y = df.drop([c for c in df.columns if c.startswith('x')], axis=1).values\n",
    "        class_values = None\n",
    "    else:\n",
    "        normalized_df = parse_dataframe(df, class_column, normalize='minmax')\n",
    "        class_values = pd.get_dummies(df[class_column]).columns.values\n",
    "\n",
    "    if args.nn[0].endswith('.txt'):\n",
    "        architecture = args.nn[0]\n",
    "    else:\n",
    "        architecture = [int(n) for n in args.nn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função de ativação\n",
    "\n",
    "def sig(x, derivative=False):\n",
    "\tif derivative:\n",
    "\t\treturn sig(x)*(1-sig(x))\n",
    "\t\n",
    "\telse:\n",
    "\t\treturn 1.0 / (1+np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "from backpropagation.util import to_one_hot\n",
    "import copy\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1. / (1. + np.exp(-z))\n",
    "\n",
    "\n",
    "class NN:\n",
    "\n",
    "    def __init__(self, architecture, regularization_factor=0.0, initial_weights=None, alpha=0.001, optimizer='Adam', \n",
    "                 beta=0.9, class_column=None, class_values=None, epochs=100, batch_size=None):\n",
    "        \"\"\"Feedforward Neural Network\n",
    "        \n",
    "        Args:\n",
    "            architecture (list/str): List of number of neurons per layer or string with .txt file.\n",
    "            regularization_factor (int, optional): Regularization Factor. Defaults to 0.\n",
    "            initial_weights (str, optional): txt file with initial weights. If None, weights are sampled from N(0,1). Defaults to None.\n",
    "            alpha (float, optional): Learning rate. Defaults to 0.001.\n",
    "            optimizer (str, optional): Which optimizer to user. ['SGD', 'Momentum', 'Adam']\n",
    "            beta (float, optional): Efective direction rate used on the Momentum Method. Defaults to 0.9.\n",
    "            epochs (int, optional): Number of training epochs. Defaults to 100.\n",
    "            batch_size (int, optional): Size of mini-batch. Defaults to size of the complete dataset.\n",
    "        \"\"\"\n",
    "        if type(architecture) is str:\n",
    "            self.build_architecture_from_file(architecture)\n",
    "        else:\n",
    "            self.architecture = architecture\n",
    "            self.regularization_factor = regularization_factor\n",
    "\n",
    "        self.initial_weights = initial_weights\n",
    "        self.alpha = alpha\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "        self.reset()\n",
    "\n",
    "        if self.optimizer == 'Momentum':\n",
    "            self.beta = beta\n",
    "            self.apply_grads = self.apply_grads_with_momentum_method\n",
    "        elif self.optimizer == 'SGD':\n",
    "            self.apply_grads = self.apply_grads_with_sgd\n",
    "        elif self.optimizer == 'Adam':\n",
    "            self.apply_grads = self.apply_grads_with_adam\n",
    "        else:\n",
    "            exit(\"Optimizer not found!\")\n",
    "\n",
    "        self.class_column = class_column\n",
    "        self.class_values = class_values\n",
    "\n",
    "    def reset(self):\n",
    "        self.init_activations()\n",
    "        self.init_deltas()\n",
    "        self.init_grads()\n",
    "        self.init_weights()\n",
    "        if self.optimizer == 'Momentum':\n",
    "            self.init_z_directions()\n",
    "        elif self.optimizer == 'Adam':\n",
    "            self.init_adam()\n",
    "    \n",
    "    def predict(self, instance):\n",
    "        instance = instance.drop(labels=[self.class_column]).values\n",
    "        instance = np.array(instance, dtype='float64')\n",
    "        activations = list(self.propagate(instance))\n",
    "        max_index = activations.index(max(activations))\n",
    "        return self.class_values[max_index]\n",
    "    \n",
    "    def train(self, x, y, x_test, y_test):\n",
    "        train_loss, test_loss = [], []  \n",
    "        n = len(x)\n",
    "        batch_size = self.batch_size if self.batch_size is not None else n\n",
    "        num_batches = n // batch_size\n",
    "        batches_x = np.array_split(x, num_batches)\n",
    "        batches_y = np.array_split(y, num_batches)\n",
    "        epochs = tqdm(range(self.epochs))\n",
    "        self.reset()\n",
    "        for e in epochs:\n",
    "            epoch_loss = 0.0\n",
    "            for batch in range(num_batches):\n",
    "                sum_loss = 0.0\n",
    "                self.reset_grads()\n",
    "                true_batch_size = len(batches_x[batch])\n",
    "\n",
    "                for i in range(true_batch_size):\n",
    "                    xi, yi = batches_x[batch][i], batches_y[batch][i].reshape(-1,1)\n",
    "                    fx = self.propagate(xi)\n",
    "                    sum_loss += self.cost(fx, yi)\n",
    "                    self.backpropagate(fx, yi)\n",
    "\n",
    "                epoch_loss += sum_loss\n",
    "                self.add_regularization_to_grads(true_batch_size)\n",
    "                self.apply_grads()\n",
    "\n",
    "            testloss = 0.0\n",
    "            for i in range(len(x_test)):\n",
    "                xi, yi = x_test[i], y_test[i].reshape(-1,1)\n",
    "                fx = self.propagate(xi)\n",
    "                testloss += self.cost(fx, yi)\n",
    "            test_loss.append(testloss/len(x_test) + self.regularization_cost(len(x_test)))\n",
    "            train_loss.append(epoch_loss/n + self.regularization_cost(n))\n",
    "\n",
    "            epochs.set_description('Epoch {}: train loss = {:.5f} test loss = {:.5f}'.format(e+1, train_loss[-1], test_loss[-1]))\n",
    "\n",
    "        return train_loss, test_loss\n",
    "\n",
    "    def train_numerically(self, x, y):\n",
    "        n = len(x)\n",
    "        self.reset()\n",
    "        for e in range(self.epochs):\n",
    "            print('\\nEpoch {}:'.format(e+1))\n",
    "            self.reset_grads()\n",
    "            print('Gradientes numericos:')\n",
    "            for i in range(n):\n",
    "                xi, yi = x[i], y[i].reshape(-1,1)\n",
    "                self.calculate_numerical_gradients(xi, yi)\n",
    "            self.add_regularization_to_grads(n)\n",
    "            print(self.gradients_as_strings())\n",
    "            numerical_grads = copy.deepcopy(self.grads)\n",
    "            self.reset_grads()\n",
    "            print('\\nGradientes backpropagation:')\n",
    "            for i in range(n):\n",
    "                xi, yi = x[i], y[i].reshape(-1,1)\n",
    "                fx = self.propagate(xi)\n",
    "                self.backpropagate(fx, yi)\n",
    "            self.add_regularization_to_grads(n)\n",
    "            print(self.gradients_as_strings(), '\\n')\n",
    "            self.print_grad_diff(numerical_grads, self.grads)\n",
    "            self.apply_grads()\n",
    "\n",
    "    def print_grad_diff(self, numerical, backpropagation):\n",
    "        for theta in range(len(numerical)):\n",
    "            mean_diff = np.mean(np.abs(numerical[theta] - backpropagation[theta]))\n",
    "            print('Erro entre grandiente via backprop e grandiente numerico para Theta%d: %.10f' %(theta+1, mean_diff))\n",
    "\n",
    "    def calculate_numerical_gradients(self, x, y):\n",
    "        epsilon = 1e-8\n",
    "        for layer in range(len(self.weights)):\n",
    "            i, dims = self.weights[layer].shape\n",
    "            for neuron in range(dims):\n",
    "                for next_layer in range(i):\n",
    "                    # Cost summing epsilon\n",
    "                    self.weights[layer][next_layer, neuron] += epsilon\n",
    "                    plus_epsilon_propagation = self.propagate(x)\n",
    "                    plus_epsilon_cost = self.cost(plus_epsilon_propagation, y)\n",
    "                    # Cost subtracting epsilon\n",
    "                    self.weights[layer][next_layer, neuron] -= (2 * epsilon)\n",
    "                    less_epsilon_propagation = self.propagate(x)\n",
    "                    less_epsilon_cost = self.cost(less_epsilon_propagation, y)\n",
    "                    # Correct weight to initial value\n",
    "                    self.weights[layer][next_layer, neuron] += epsilon\n",
    "                    # Calculate gradient\n",
    "                    gradient = (plus_epsilon_cost - less_epsilon_cost) / (2 * epsilon)\n",
    "                    # Update gradient\n",
    "                    self.grads[layer][next_layer][neuron] += gradient\n",
    "\n",
    "    def backpropagate(self, fx, y):\n",
    "        \"\"\"Computes gradients using backpropagation\n",
    "        Args:\n",
    "            fx (np.array): NN predictions\n",
    "            y (np.array): True outputs\n",
    "        \"\"\"\n",
    "        self.calculate_deltas(fx, y)\n",
    "        self.calcutate_grads()\n",
    "\n",
    "    def calculate_deltas(self, fx, y):\n",
    "        # Set output layer separately\n",
    "        np.subtract(fx, y, out=self.deltas[-1])\n",
    "        for layer in range(self.num_layers-2, 0, -1):\n",
    "            weights = self.weights[layer].transpose()[1:]\n",
    "            uncertainty = np.multiply(self.activations[layer][1:], (1 - self.activations[layer][1:]))\n",
    "            np.multiply(np.dot(weights, self.deltas[layer]), uncertainty, out=self.deltas[layer-1])\n",
    "\n",
    "    def propagate(self, x):\n",
    "        \"\"\"Propagates forward an instance, computing the activation of each neuron\n",
    "        Args:\n",
    "            x (np.array): Instance\n",
    "        Returns:\n",
    "            np.array: The output layer return\n",
    "        \"\"\"\n",
    "        np.copyto(self.activations[0], np.append(1.0, x).reshape(-1,1))\n",
    "        for layer in range(1, self.num_layers-1):\n",
    "            np.dot(self.weights[layer-1], self.activations[layer-1], out=self.activations[layer][1:])\n",
    "            self.activations[layer] = sigmoid(self.activations[layer])\n",
    "            self.activations[layer][0][0] = 1.0\n",
    "        np.dot(self.weights[self.num_layers-2], self.activations[self.num_layers-2], out=self.activations[self.num_layers-1])\n",
    "        self.activations[self.num_layers-1] = sigmoid(self.activations[self.num_layers-1])\n",
    "        return self.activations[self.num_layers-1]\n",
    "\n",
    "    def cost(self, fx, y):\n",
    "        \"\"\"Cross-entropy loss\n",
    "        Args:\n",
    "            fx (np.array): NN instance predictions\n",
    "            y (np.array): True outputs\n",
    "        Returns:\n",
    "            float: Logistic loss\n",
    "        \"\"\"\n",
    "        j = -((y * np.log(fx) + (1 - y) * np.log(1 - fx)))\n",
    "        return j.sum()\n",
    "\n",
    "    def regularization_cost(self, num_examples):\n",
    "        return (self.regularization_factor/(2*num_examples)) * np.sum(np.square(w[:, 1:]).sum() for w in self.weights)\n",
    "\n",
    "    def calcutate_grads(self):\n",
    "        \"\"\"Computes the gradient for each weight\n",
    "        \"\"\"\n",
    "        for i in range(self.num_layers-2, -1, -1):\n",
    "            grad = np.dot(self.deltas[i], self.activations[i].reshape(1,-1))\n",
    "            self.grads[i] += grad\n",
    "\n",
    "    def apply_grads_with_sgd(self):\n",
    "        self.weights -= self.alpha * self.grads\n",
    "\n",
    "    def apply_grads_with_momentum_method(self):\n",
    "        self.z_directions *= self.beta\n",
    "        self.z_directions += self.grads\n",
    "        self.weights -= self.alpha * self.z_directions\n",
    "    \n",
    "    def apply_grads_with_adam(self):\n",
    "        self.t += 1\n",
    "        beta_1 = 0.9\n",
    "        beta_2 = 0.999\n",
    "        epsilon = 1e-8\n",
    "\n",
    "        self.m_t = beta_1 * self.m_t + (1-beta_1) * self.grads\t      # updates the moving averages of the gradient\n",
    "        self.v_t = beta_2 * self.v_t + (1-beta_2) * (self.grads**2)\t  # updates the moving averages of the squared gradient\n",
    "\n",
    "        m_cap = self.m_t/(1-(beta_1**self.t))\t\t# calculates the bias-corrected estimates\n",
    "        v_cap = self.v_t/(1-(beta_2**self.t))\t\t# calculates the bias-corrected estimates\n",
    "        sqrt_v = np.array([np.sqrt(x) for x in v_cap])\n",
    "        self.weights -= (self.alpha*m_cap) / (sqrt_v + epsilon)\n",
    "\n",
    "    def init_adam(self):\n",
    "        self.t = 0\n",
    "        self.m_t = np.array([np.zeros((self.architecture[layer+1], self.architecture[layer]+1), dtype='float') for layer in range(self.num_layers-1)])\n",
    "        self.v_t = np.array([np.zeros((self.architecture[layer+1], self.architecture[layer]+1), dtype='float') for layer in range(self.num_layers-1)])\n",
    "\n",
    "    def add_regularization_to_grads(self, num_examples):\n",
    "        \"\"\"Sums the regularization times the weights to the gradients, and computes the mean gradient\n",
    "        Args:\n",
    "            num_examples (int): Number of instances used to compute the gradient\n",
    "        \"\"\"\n",
    "        for i in range(self.num_layers-2, -1, -1):\n",
    "            p = self.weights[i].copy()\n",
    "            p[:, 0] = 0  # ignore bias weights\n",
    "            self.grads[i] += self.regularization_factor * p\n",
    "            self.grads[i] /= num_examples\n",
    "\n",
    "    @property\n",
    "    def num_layers(self):\n",
    "        return len(self.architecture)\n",
    "    \n",
    "    def reset_grads(self):\n",
    "        for g in self.grads:\n",
    "            g.fill(0.0)\n",
    "\n",
    "    def init_activations(self):\n",
    "        self.activations = []\n",
    "        for layer in range(self.num_layers-1):\n",
    "            self.activations.append(np.empty((self.architecture[layer]+1,1)))  # column vector with bias\n",
    "            self.activations[layer][0][0] = 1.0  # bias neuron\n",
    "        self.activations.append(np.empty((self.architecture[-1],1))) # output layer doesn't have bias\n",
    "\n",
    "    def init_weights(self):\n",
    "        if self.initial_weights is None:\n",
    "            self.init_random_weights()\n",
    "        else:\n",
    "            self.read_weights_from_file(self.initial_weights)\n",
    "    \n",
    "    def init_random_weights(self):\n",
    "        self.weights = np.array([np.random.normal(size=(self.architecture[layer+1], self.architecture[layer]+1)) for layer in range(self.num_layers-1)])\n",
    "\n",
    "    def read_weights_from_file(self, file):\n",
    "        w = []\n",
    "        with open(file, 'r') as f:\n",
    "            for line in f:\n",
    "                w.append([float(x) for x in line.replace(';',',').split(',')])\n",
    "        self.weights = np.array([np.asmatrix(w[layer]).reshape(self.architecture[layer+1], self.architecture[layer]+1) for layer in range(self.num_layers-1)])\n",
    "\n",
    "    def init_deltas(self):\n",
    "        self.deltas = np.array([np.empty((self.architecture[n], 1)) for n in range(1, self.num_layers)])\n",
    "\n",
    "    def init_z_directions(self):\n",
    "        self.z_directions = np.array([np.zeros(layer.shape) for layer in self.weights])\n",
    "\n",
    "    def init_grads(self):\n",
    "        self.grads = np.array([np.zeros((self.architecture[layer+1], self.architecture[layer]+1)) for layer in range(self.num_layers-1)])\n",
    "\n",
    "    def build_architecture_from_file(self, architecture):\n",
    "        with open(architecture, 'r') as f:\n",
    "            fileread = f.read().splitlines()\n",
    "        self.regularization_factor = float(fileread[0])\n",
    "        self.architecture = [int (x) for x in fileread[1:]]\n",
    "\n",
    "    def gradients_as_strings(self):\n",
    "        return \"\\n\".join(\n",
    "            [\"; \".join(\n",
    "                [\", \".join([\"{:.5f}\".format(w) for w in neuron])\n",
    "                 for neuron in layer]\n",
    "            ) for layer in self.grads]\n",
    "        )"
   ]
  }
 ]
}